# ELT Builder - Complete Project Overview

## ğŸ‰ Project Successfully Rebuilt!

This document provides a complete overview of the ELT Builder project after the full rebuild.

---

## ğŸ“¦ What's Included

### 1. **embedded_elt_builder/** - Core Package
The main Python package for creating and managing ELT pipelines.

**CLI Tool (`elt` command):**
```bash
elt scaffold create <name> --source <type> --destination <type>
elt list
elt delete <name>
elt ui
```

**Web UI:**
```bash
elt ui
# Opens at http://127.0.0.1:8000
```

**Features:**
- âœ… 25+ data sources (GitHub, Stripe, Salesforce, Postgres, etc.)
- âœ… 15+ destinations (Snowflake, BigQuery, Redshift, DuckDB, etc.)
- âœ… Smart tool selection (automatically chooses dlt or Sling)
- âœ… Source configuration (beyond credentials - what data to load)
- âœ… Interactive CLI with multiselect, boolean, text prompts
- âœ… Modern web UI with dynamic forms
- âœ… Git integration (auto-commit and push)

### 2. **dagster_elt_project/** - Dagster Orchestration
Automated Dagster integration that discovers and orchestrates your pipelines.

**Features:**
- âœ… Auto-discovery of pipelines from `pipelines/` directory
- âœ… Asset-based architecture
- âœ… Scheduling support (cron expressions)
- âœ… Grouping by asset groups
- âœ… Full observability through Dagster UI

**Usage:**
```bash
cd dagster_elt_project
pip install -e .
dagster dev
# Open http://localhost:3000
```

### 3. **elt_pipelines_example/** - Sample Project
A complete, standalone example project with working pipelines.

**Includes:**
- âœ… GitHub Issues pipeline (dlt)
- âœ… Postgres to DuckDB replication (Sling)
- âœ… Configuration files for Dagster
- âœ… Run script for easy testing
- âœ… Comprehensive README

**Usage:**
```bash
cd elt_pipelines_example
cp .env.example .env
# Edit .env with your credentials
./run_pipeline.sh
```

---

## ğŸ—ï¸ Architecture

```
embedded_elt_builder/
â”œâ”€â”€ embedded_elt_builder/           # Core package
â”‚   â”œâ”€â”€ cli/                        # CLI commands
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ scaffold.py
â”‚   â”‚   â”œâ”€â”€ list_pipelines.py
â”‚   â”‚   â”œâ”€â”€ delete.py
â”‚   â”‚   â””â”€â”€ ui.py
â”‚   â”œâ”€â”€ web/                        # Web UI
â”‚   â”‚   â”œâ”€â”€ app_enhanced.py         # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ credentials_config.py   # Source/dest configs
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â””â”€â”€ index_enhanced.html
â”‚   â”‚   â””â”€â”€ __main__.py
â”‚   â””â”€â”€ pipeline_generator.py       # Shared pipeline generation
â”‚
â”œâ”€â”€ dagster_elt_project/            # Dagster orchestration
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ dlt_component.py        # Auto-discover dlt pipelines
â”‚   â”‚   â””â”€â”€ sling_component.py      # Auto-discover Sling replications
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ dagster.yaml.schema
â”‚   â”‚   â””â”€â”€ config.yaml.schema
â”‚   â””â”€â”€ __init__.py
â”‚
â””â”€â”€ elt_pipelines_example/          # Sample project
    â”œâ”€â”€ pipelines/
    â”‚   â”œâ”€â”€ dlt/
    â”‚   â”‚   â””â”€â”€ github_issues/
    â”‚   â”‚       â”œâ”€â”€ pipeline.py
    â”‚   â”‚       â”œâ”€â”€ dagster.yaml
    â”‚   â”‚       â””â”€â”€ config.yaml
    â”‚   â””â”€â”€ sling/
    â”‚       â””â”€â”€ postgres_to_duckdb/
    â”‚           â”œâ”€â”€ replication.yaml
    â”‚           â”œâ”€â”€ dagster.yaml
    â”‚           â””â”€â”€ config.yaml
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ run_pipeline.sh
    â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start Guide

### Step 1: Install the Core Package

```bash
cd embedded_elt_builder
pip install -e .
```

### Step 2: Try the CLI

```bash
# Interactive pipeline creation
elt scaffold create my_pipeline --source github --destination snowflake

# List pipelines
elt list

# Launch web UI
elt ui
```

### Step 3: Try the Example Project

```bash
cd elt_pipelines_example

# Set up environment
cp .env.example .env
nano .env  # Add your credentials

# Run a pipeline
./run_pipeline.sh
```

### Step 4: Add Dagster Orchestration

```bash
cd dagster_elt_project
pip install -e .
dagster dev
```

Open http://localhost:3000 and see your pipelines as Dagster assets!

---

## ğŸ“ Configuration Files

Every pipeline has three configuration files:

### 1. Pipeline Code/Config
- **dlt**: `pipeline.py` - Python code
- **Sling**: `replication.yaml` - YAML configuration

### 2. dagster.yaml (Dagster Configuration)
```yaml
enabled: true
description: "Pipeline description"
group: "asset_group"

schedule:
  enabled: true
  cron: "0 2 * * *"
  timezone: "UTC"

owners:
  - "team@company.com"

retries: 3
retry_delay: 60
```

### 3. config.yaml (Pipeline Metadata)
```yaml
source_type: "github"
destination_type: "snowflake"

source_configuration:
  repos: "dlt-hub/dlt"
  resources: ["issues", "pull_requests"]

tool: "dlt"
version: "1.0.0"
```

---

## ğŸ¯ Common Workflows

### Create a New Pipeline

**Using CLI (Interactive):**
```bash
elt scaffold create stripe_to_snowflake \
  --source stripe \
  --destination snowflake
```

**Using Web UI:**
```bash
elt ui
# Use the form to create pipelines
```

### Run Pipelines Standalone

**dlt pipeline:**
```bash
cd pipelines/dlt/my_pipeline
python pipeline.py
```

**Sling replication:**
```bash
cd pipelines/sling/my_replication
sling run -r replication.yaml
```

### Run with Dagster

```bash
cd dagster_elt_project
dagster dev
# Materialize assets in the UI
```

---

## ğŸ”§ Supported Sources & Destinations

### Sources (25+)
- **APIs**: GitHub, Stripe, Shopify, Salesforce, HubSpot, Slack, Notion, Zendesk, Jira
- **Databases**: Postgres, MySQL, MongoDB, DuckDB, SQLite
- **Cloud Storage**: S3, GCS, Azure Blob
- **Analytics**: Google Analytics, Mixpanel, Segment
- **Files**: CSV, JSON, Parquet

### Destinations (15+)
- **Cloud Warehouses**: Snowflake, BigQuery, Redshift, Databricks
- **Databases**: Postgres, MySQL, DuckDB, MotherDuck, ClickHouse
- **Local**: SQLite, Filesystem

---

## ğŸ¨ Source Configuration

Beyond credentials, configure **what data** to load:

**GitHub:**
- Which repositories
- Which resources (issues, PRs, commits)

**Stripe:**
- Which resources (customers, invoices, charges)

**Postgres:**
- Which schemas and tables
- Incremental vs full refresh

**And more!**

---

## ğŸ“š Directory Structure

### Your ELT Repository
```
my-elt-pipelines/
â”œâ”€â”€ .env                    # Credentials (git-ignored)
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ dlt/
â”‚   â”‚   â”œâ”€â”€ github_issues/
â”‚   â”‚   â”œâ”€â”€ stripe_charges/
â”‚   â”‚   â””â”€â”€ salesforce_accounts/
â”‚   â””â”€â”€ sling/
â”‚       â”œâ”€â”€ postgres_to_snowflake/
â”‚       â””â”€â”€ mysql_to_bigquery/
â””â”€â”€ README.md
```

### With Dagster
```
my-elt-pipelines/
â”œâ”€â”€ .env
â”œâ”€â”€ pipelines/              # Your pipelines
â”œâ”€â”€ dagster_elt_project/    # Copy from this repo
â””â”€â”€ pyproject.toml
```

---

## ğŸ§ª Testing

### Test the CLI
```bash
# Create a test pipeline
elt scaffold create test_pipeline \
  --source github \
  --destination duckdb \
  --no-interactive

# Verify it was created
elt list

# Delete it
elt delete test_pipeline
```

### Test the Web UI
```bash
# Start the UI
elt ui

# Open http://127.0.0.1:8000
# Create, view, and delete pipelines through the UI
```

### Test Example Pipelines
```bash
cd elt_pipelines_example
cp .env.example .env
# Add GITHUB_TOKEN to .env
cd pipelines/dlt/github_issues
python pipeline.py
```

---

## ğŸ“ Learning Resources

### dlt (Python ELT)
- [dlt Documentation](https://dlthub.com/docs)
- [Verified Sources](https://dlthub.com/docs/dlt-ecosystem/verified-sources)
- [Custom Sources Tutorial](https://dlthub.com/docs/tutorial/load-data-from-an-api)

### Sling (YAML-based Replication)
- [Sling Documentation](https://docs.slingdata.io)
- [Connectors](https://docs.slingdata.io/connections)
- [Replication Config](https://docs.slingdata.io/sling-cli/run)

### Dagster (Orchestration)
- [Dagster Docs](https://docs.dagster.io)
- [Software-Defined Assets](https://docs.dagster.io/concepts/assets/software-defined-assets)
- [Schedules & Sensors](https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules)

---

## ğŸš¨ Troubleshooting

### CLI Not Found
```bash
pip install -e embedded_elt_builder/
```

### Web UI Port Conflict
```bash
elt ui --port 3000
```

### Dagster Not Finding Pipelines
- Ensure `enabled: true` in `dagster.yaml`
- Check that all required files exist
- Refresh Dagster UI (reload definitions)

### Pipeline Failing
- Verify credentials in `.env`
- Check pipeline-specific logs
- Test data source connection manually

---

## ğŸ‰ What Makes This Special

1. **Unified Interface** - CLI and Web UI share the same pipeline generation logic
2. **Smart Tool Selection** - Automatically chooses dlt or Sling based on source/destination
3. **Beyond Credentials** - Configure what data to load, not just how to connect
4. **Dagster Ready** - Pipelines work standalone but are better with orchestration
5. **Production Ready** - Git integration, scheduling, monitoring, error handling
6. **Comprehensive** - 25+ sources, 15+ destinations, fully documented

---

## ğŸ“„ Files Reference

### Key Files in embedded_elt_builder/
- `embedded_elt_builder/pipeline_generator.py` - Core pipeline generation logic
- `embedded_elt_builder/cli/scaffold.py` - Interactive CLI pipeline creation
- `embedded_elt_builder/web/app_enhanced.py` - FastAPI backend
- `embedded_elt_builder/web/credentials_config.py` - Source/destination configs
- `embedded_elt_builder/web/templates/index_enhanced.html` - Web UI frontend

### Key Files in dagster_elt_project/
- `dagster_elt_project/__init__.py` - Main Dagster definitions
- `dagster_elt_project/components/dlt_component.py` - dlt pipeline discovery
- `dagster_elt_project/components/sling_component.py` - Sling replication discovery

### Key Files in elt_pipelines_example/
- `elt_pipelines_example/pipelines/dlt/github_issues/pipeline.py` - Example dlt pipeline
- `elt_pipelines_example/pipelines/sling/postgres_to_duckdb/replication.yaml` - Example Sling config
- `elt_pipelines_example/run_pipeline.sh` - Quick run script

---

## ğŸ¯ Next Steps

1. **Explore the example project** - Run the sample pipelines
2. **Create your first pipeline** - Use CLI or Web UI
3. **Set up Dagster** - Get orchestration and monitoring
4. **Customize for your needs** - Add your data sources
5. **Deploy to production** - Use Dagster Cloud or self-hosted

---

## ğŸ’¡ Pro Tips

- Use the **Web UI** for visual pipeline creation
- Use the **CLI** for automation and scripting
- Use **Dagster** for production orchestration
- Store credentials in `.env` files (never commit!)
- Use `dagster.yaml` to control scheduling
- Group related pipelines with `group` field
- Add `owners` for accountability
- Enable only the pipelines you need

---

## ğŸ™ Support

For questions or issues:
- Check the README files in each directory
- Review the example pipelines
- Check dlt/Sling/Dagster documentation
- Inspect configuration schemas

---

**Happy data engineering! ğŸš€**
